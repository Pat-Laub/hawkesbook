# -*- coding: utf-8 -*-
import numpy as np
import numpy.random as rnd
from scipy.optimize import fsolve, minimize

from tqdm import tqdm
from numba import njit, prange


@njit()
def numba_seed(seed):
    rnd.seed(seed)


# Intensities and compensators


def hawkes_intensity(t, â„‹_t, ğ›‰):
    Î», Î¼, _ = ğ›‰
    Î»Ë£ = Î»
    for t_i in â„‹_t:
        Î»Ë£ += Î¼(t - t_i)
    return Î»Ë£


def hawkes_compensator(t, â„‹_t, ğ›‰):
    if t <= 0: return 0
    Î», _, M = ğ›‰

    Î› = Î» * t
    for t_i in â„‹_t:
        Î› += M(t - t_i)
    return Î›


def exp_hawkes_intensity(t, â„‹_t, ğ›‰):
    Î», Î±, Î² = ğ›‰
    Î»Ë£ = Î»
    for t_i in â„‹_t:
        Î»Ë£ += Î± * np.exp(-Î² * (t - t_i))
    return Î»Ë£


def exp_hawkes_compensator(t, â„‹_t, ğ›‰):
    if t <= 0: return 0
    Î», Î±, Î² = ğ›‰
    Î› = Î» * t
    for t_i in â„‹_t:
        Î› += (Î±/Î²) * (1 - np.exp(-Î²*(t - t_i)))
    return Î›


@njit(nogil=True)
def exp_hawkes_compensators(â„‹_t, ğ›‰):
    Î», Î±, Î² = ğ›‰

    Î› = 0
    Î»Ë£_prev = Î»
    t_prev = 0

    Î›s = np.empty(len(â„‹_t), dtype=np.float64)
    for i, t_i in enumerate(â„‹_t):
        Î› += Î» * (t_i - t_prev) + (
                (Î»Ë£_prev - Î»)/Î² *
                (1 - np.exp(-Î²*(t_i - t_prev))))
        Î›s[i] = Î›

        Î»Ë£_prev = Î» + (Î»Ë£_prev - Î») * (
                np.exp(-Î² * (t_i - t_prev))) + Î±
        t_prev = t_i
    return Î›s


@njit(nogil=True)
def power_hawkes_intensity(t, â„‹_t, ğ›‰):
    Î», k, c, p = ğ›‰
    Î»Ë£ = Î»
    for t_i in â„‹_t:
        Î»Ë£ += k / (c + (t-t_i))**p
    return Î»Ë£


@njit(nogil=True)
def power_hawkes_compensator(t, â„‹_t, ğ›‰):
    Î», k, c, p = ğ›‰
    Î› = Î» * t
    for t_i in â„‹_t:
        Î› += ((k * (c * (c + (t-t_i)))**-p *
              (-c**p * (c + (t-t_i)) + c * (c + (t-t_i))**p)) /
              (p - 1))
    return Î›


@njit(nogil=True, parallel=True)
def power_hawkes_compensators(â„‹_t, ğ›‰):
    Î›s = np.empty(len(â„‹_t), dtype=np.float64)
    for i in prange(len(â„‹_t)):
        t_i = â„‹_t[i]
        â„‹_i = â„‹_t[:i]
        Î›s[i] = power_hawkes_compensator(t_i, â„‹_i, ğ›‰)
    return Î›s


# Likelihood

def log_likelihood(â„‹_T, T, ğ›‰, Î»Ë£, Î›):
    â„“ = 0.0
    for i, t_i in enumerate(â„‹_T):
        â„‹_i = â„‹_T[:i]
        Î»Ë£_i = Î»Ë£(t_i, â„‹_i, ğ›‰)
        â„“ += np.log(Î»Ë£_i)
    â„“ -= Î›(T, â„‹_T, ğ›‰)
    return â„“


@njit(nogil=True, parallel=True)
def power_log_likelihood(â„‹_T, T, ğ›‰):
    â„“ = 0.0
    for i in prange(len(â„‹_T)):
        t_i = â„‹_T[i]
        â„‹_i = â„‹_T[:i]
        Î»Ë£_i = power_hawkes_intensity(t_i, â„‹_i, ğ›‰)
        â„“ += np.log(Î»Ë£_i)
    â„“ -= power_hawkes_compensator(T, â„‹_T, ğ›‰)
    return â„“


@njit()
def exp_log_likelihood(â„‹_T, T, ğ›‰):
    Î», Î±, Î² = ğ›‰
    ğ­ = â„‹_T
    N_T = len(ğ­)

    A = np.empty(N_T, dtype=np.float64)
    A[0] = 0
    for i in range(1, N_T):
        A[i] = np.exp(-Î²*(ğ­[i] - ğ­[i-1])) * (1 + A[i-1])

    â„“ = -Î»*T
    for i, t_i in enumerate(â„‹_T):
        â„“ += np.log(Î» + Î± * A[i]) - \
                (Î±/Î²) * (1 - np.exp(-Î²*(T-t_i)))
    return â„“


def exp_mle(ğ­, T, ğ›‰_start=np.array([1.0, 2.0, 3.0])):
    eps = 1e-5
    ğ›‰_bounds = ((eps, None), (eps, None), (eps, None))
    loss = lambda ğ›‰: -exp_log_likelihood(ğ­, T, ğ›‰)
    ğ›‰_mle = minimize(loss, ğ›‰_start, bounds=ğ›‰_bounds).x
    return np.array(ğ›‰_mle)


def power_mle(ğ­, T, ğ›‰_start=np.array([1.0, 1.0, 2.0, 3.0])):
    eps = 1e-5
    ğ›‰_bounds = ((eps, None), (eps, None), (eps, None),
        (1+eps, 100))
    loss = lambda ğ›‰: -power_log_likelihood(ğ­, T, ğ›‰)
    ğ›‰_mle = minimize(loss, ğ›‰_start, bounds=ğ›‰_bounds).x
    return np.array(ğ›‰_mle)


# Simulation


def simulate_inverse_compensator(ğ›‰, Î›, N):
    â„‹ = np.empty(N, dtype=np.float64)

    tË£_1 = -np.log(rnd.rand())
    exp_1 = lambda t_1: Î›(t_1, â„‹[:0], ğ›‰) - tË£_1

    t_1_guess = 1.0
    t_1 = fsolve(exp_1, t_1_guess)[0]

    â„‹[0] = t_1
    t_prev = t_1
    for i in range(1, N):
        Î”tË£_i = -np.log(rnd.rand())

        Î›_i = Î›(t_prev, â„‹, ğ›‰)
        exp_i = lambda t_next: Î›(t_next, â„‹[:i], ğ›‰) - Î›_i - Î”tË£_i

        t_next_guess = t_prev + 1.0
        t_next = fsolve(exp_i, t_next_guess)[0]

        â„‹[i] = t_next
        t_prev = t_next
    return â„‹

@njit(nogil=True)
def exp_simulate_by_composition(ğ›‰, N):
    Î», Î±, Î² = ğ›‰
    Î»Ë£_k = Î»
    t_k = 0

    â„‹ = np.empty(N, dtype=np.float64)
    for k in range(N):
        U_1 = rnd.rand()
        U_2 = rnd.rand()

        T_1 = t_k - np.log(U_1) / Î»
        T_2 = t_k - np.log(1 + Î²/(Î»Ë£_k + Î± - Î»)*np.log(U_2))/Î²

        t_prev = t_k
        t_k = min(T_1, T_2)
        â„‹[k] = t_k

        Î»Ë£_k = Î» + (Î»Ë£_k + Î± - Î») * (
                np.exp(-Î² * (t_k - t_prev)))
    return â„‹


@njit(nogil=True)
def exp_simulate_by_thinning(ğ›‰, T):
    Î», Î±, Î² = ğ›‰

    Î»Ë£ = Î»
    times = []

    t = 0

    while True:
        M = Î»Ë£
        Î”t = rnd.exponential() / M
        t += Î”t
        if t > T:
            break

        Î»Ë£ = Î» + (Î»Ë£ - Î») * np.exp(-Î² * Î”t)

        u = M * rnd.rand()
        if u > Î»Ë£:
            continue  # This potential arrival is 'thinned' out

        times.append(t)
        Î»Ë£ += Î±

    return np.array(times)


@njit(nogil=True)
def power_simulate_by_thinning(ğ›‰, T):
    Î», k, c, p = ğ›‰

    Î»Ë£ = Î»
    times = []

    t = 0

    while True:
        M = Î»Ë£
        Î”t = rnd.exponential() / M
        t += Î”t
        if t > T:
            break

        Î»Ë£ = power_hawkes_intensity(t, np.array(times), ğ›‰)

        u = M * rnd.rand()
        if u > Î»Ë£:
            continue  # This potential arrival is 'thinned' out

        times.append(t)
        Î»Ë£ += k / (c ** p)

    return np.array(times)


# Moment matching


def empirical_moments(ğ­, T, Ï„, lag):
    bins = np.arange(0, T, Ï„)
    N = len(bins) - 1
    count = np.zeros(N)

    for i in range(N):
        count[i] = np.sum((bins[i] <= ğ­) & (ğ­ < bins[i+1]))

    empMean = np.mean(count)
    empVar = np.std(count)**2
    empAutoCov = np.mean((count[:-lag] - empMean) \
                    * (count[lag:] - empMean))

    return np.array([empMean, empVar, empAutoCov]).reshape(3,1)



def exp_moments(ğ›‰, Ï„, lag):
    """
    Consider an exponential Hawkes process with parameter ğ›‰.
    Look at intervals of length Ï„, i.e. N(t+Ï„) - N(t).
    Calculate the limiting (t->âˆ) mean and variance.
    Also, get the limiting autocovariance:
        E[ (N(t + Ï„) - N(t)) (N(t + lag*Ï„ + Ï„) - N(t + lag*Ï„)) ].
    """
    Î», Î±, Î² = ğ›‰
    Îº = Î² - Î±
    Î´ = lag*Ï„

    mean = (Î»*Î²/Îº)*Ï„
    var = (Î»*Î²/Îº)*(Ï„*(Î²/Îº) + (1 - Î²/Îº)*((1 - np.exp(-Îº*Ï„))/Îº))
    autoCov = (Î»*Î²*Î±*(2*Î²-Î±)*(np.exp(-Îº*Ï„) - 1)**2/(2*Îº**4)) \
                *np.exp(-Îº*Î´)

    return np.array([mean, var, autoCov]).reshape(3,1)


def exp_gmm_loss(ğ›‰, Ï„, lag, empMoments, W):
    moments = exp_moments(ğ›‰, Ï„, lag)
    ğ  = empMoments - moments
    return (ğ .T).dot(W).dot(ğ )[0,0]

def exp_gmm(ğ­, T, Ï„=5, lag=5, iters=2, ğ›‰_start=np.array([1.0, 2.0, 3.0])):
    empMoments = empirical_moments(ğ­, T, Ï„, lag)

    W = np.eye(3)
    bounds = ((0, None), (0, None), (0, None))

    ğ›‰ = minimize(exp_gmm_loss, x0=ğ›‰_start,
            args=(Ï„, lag, empMoments, W),
            bounds=bounds).x

    for i in range(iters):
        moments = exp_moments(ğ›‰, Ï„, lag)

        ğ  = empMoments - moments
        S = ğ .dot(ğ .T)

        W = np.linalg.inv(S)
        W /= np.max(W) # Avoid overflow of the loss function

        ğ›‰ = minimize(exp_gmm_loss, x0=ğ›‰,
                args=(Ï„, lag, empMoments, W),
                bounds=bounds).x

    return ğ›‰


# Fit EM


@njit(nogil=True, parallel=True)
def em_responsibilities(ğ­, ğ›‰):
    Î», Î±, Î² = ğ›‰

    N = len(ğ­)
    resp = np.empty((N,N), dtype=np.float64)

    for i in prange(0,N):
        if i == 0:
            resp[i, 0] = 1.0
            for j in range(1, N):
                resp[i, j] = 0.0
        else:
            resp[i, 0] = Î»
            rowSum = Î»

            for j in range(1, i+1):
                resp[i, j] = Î±*np.exp(-Î²*(ğ­[i] - ğ­[j-1]))
                rowSum += resp[i, j]

            for j in range(0, i+1):
                resp[i, j] /= rowSum

            for j in range(i+1, N):
                resp[i, j] = 0.0
    return resp


def exp_em(ğ­, T, ğ›‰_start=np.array([1.0, 2.0, 3.0]), iters=100, verbosity=None, calcLikelihoods=False):
    """
    Run an EM fit on the 'ğ­' arrival times up until final time 'T'.

    hp.fit(array<float>, float) -> array<float>
    """
    ğ›‰ = ğ›‰_start.copy()

    llIterations = np.zeros(iters)
    iters = tqdm(range(iters)) if verbosity else range(iters)

    for i in iters:
        ğ›‰, ll = exp_em_iter(ğ­, T, ğ›‰, calcLikelihoods)
        llIterations[i] = ll

        if verbosity and i % verbosity == 0:
            print(ğ›‰[0], ğ›‰[1], ğ›‰[2])

    if calcLikelihoods:
        return ğ›‰, llIterations
    else:
        return ğ›‰


@njit(nogil=True, parallel=True)
def exp_em_iter(ğ­, T, ğ›‰, calcLikelihoods):
    Î», Î±, Î² = ğ›‰
    N = len(ğ­)

    # E step
    resp = em_responsibilities(ğ­, ğ›‰)

    # M step: Update Î»
    Î» = np.sum(resp[:,0])/T

    # M step: Update Î±
    numer = np.sum(resp[:,1:])
    denom = np.sum(1 - np.exp(-Î²*(T - ğ­)))
    Î± = Î²*numer/denom

    # M step: Update Î²
    numer = np.sum(1 - np.exp(-Î²*(T - ğ­)))/Î² - np.sum((T - ğ­)*np.exp(-Î²*(T - ğ­)))

    denom = 0
    for j in prange(1, N):
        denom += np.sum((ğ­[j] - ğ­[:j])*resp[j,1:j+1])

    Î² = Î±*numer/denom

    if calcLikelihoods:
        ll = exp_log_likelihood(ğ­, T, ğ›‰)
    else:
        ll = 0.0

    ğ›‰[0] = Î»
    ğ›‰[1] = Î±
    ğ›‰[2] = Î²

    return ğ›‰, ll


## Mutually exciting Hawkes with exponential decay
@njit()
def mutual_hawkes_intensity(t, â„‹_t, ğ›‰):
    """
    Each Î¼[i] is an m-vector-valued function, which takes as argument
    the time passed since an arrival to process i, and returns the
    lasting effect on each of the m processes
    """
    Î», Î¼ = ğ›‰

    Î»Ë£ = Î»
    for (t_i, d_i) in â„‹_t:
        Î»Ë£ += Î¼[d_i](t - t_i)
    return Î»Ë£


@njit(nogil=True)
def mutual_exp_hawkes_intensity(t, times, ids, ğ›‰):
    """
    The Î» is an m-vector which shows the starting intensity for
    each process.

    Each Î±[i] is an m-vector which shows the jump in intensity
    for each of the processes when an arrival comes to process i.

    The Î² is an m-vector which shows the intensity decay rates for
    each processes intensity.
    """
    Î», Î±, Î² = ğ›‰

    Î»Ë£ = Î».copy()
    for (t_i, d_i) in zip(times, ids):
        Î»Ë£ += Î±[d_i] * np.exp(-Î² * (t - t_i))

    return Î»Ë£


@njit(nogil=True)
def mutual_exp_hawkes_compensator(t, times, ids, ğ›‰):
    """
    The Î» is an m-vector which shows the starting intensity for
    each process.

    Each Î±[i] is an m-vector which shows the jump in intensity
    for each of the processes when an arrival comes to process i.

    The Î² is an m-vector which shows the intensity decay rates for
    each processes intensity.
    """
    # if t <= 0: return np.zeros(m)

    Î», Î±, Î² = ğ›‰

    Î› = Î» * t

    for (t_i, d_i) in zip(times, ids):
        # Î› += M(t - t_i, d_i)
        Î› += (Î±[d_i]/Î²) * (1 - np.exp(-Î²*(t - t_i)))
    return Î›


@njit(nogil=True)
def mutual_exp_hawkes_compensators(times, ids, ğ›‰):
    """
    The Î» is an m-vector which shows the starting intensity for
    each process.

    Each Î±[i] is an m-vector which shows the jump in intensity
    for each of the processes when an arrival comes to process i.

    The Î² is an m-vector which shows the intensity decay rates for
    each processes intensity.
    """

    Î», Î±, Î² = ğ›‰
    m = len(Î»)

    Î› = np.zeros(m)
    Î»Ë£_prev = Î»
    t_prev = 0

    Î›s = np.zeros((len(times), m), dtype=np.float64)

    for i in range(len(times)):
        t_i = times[i]
        d_i = ids[i]

        Î› += Î» * (t_i - t_prev) + (Î»Ë£_prev - Î»)/Î² * (1 - np.exp(-Î²*(t_i - t_prev)))
        Î›s[i,:] = Î›

        Î»Ë£_prev = Î» + (Î»Ë£_prev - Î») * np.exp(-Î² * (t_i - t_prev)) + Î±[d_i,:]
        t_prev = t_i

    return Î›s


@njit(nogil=True)
def mutual_log_likelihood(â„‹_T, T, ğ›‰, Î»Ë£, Î›):
    m = len(ğ›‰)
    â„“ = 0
    for (t_i, d_i) in â„‹_T:
        if t_i > T:
            raise RuntimeError("T is too small for this data")

        # Get the history of arrivals before time t_i
        â„‹_i = [(t_s, d_s) for (t_s, d_s) in â„‹_T if t_s < t_i]
        Î»Ë£_i = Î»Ë£(t_i, â„‹_i, ğ›‰)
        â„“ += np.log(Î»Ë£_i[d_i])

    â„“ -= np.sum(Î›(T, â„‹_T, ğ›‰))
    return â„“


@njit(nogil=True)
def mutual_exp_log_likelihood(times, ids, T, ğ›‰):
    if np.max(times) > T:
        raise RuntimeError("T is too small for this data")

    Î», Î±, Î² = ğ›‰

    if np.min(Î») <= 0 or np.min(Î±) < 0 or np.min(Î²) <= 0: return -np.inf

    â„“ = 0
    Î»Ë£ = ğ›‰[0]

    t_prev = 0
    for t_i, d_i in zip(times, ids):
        Î»Ë£ = Î» + (Î»Ë£ - Î») * np.exp(-Î² * (t_i - t_prev))
        â„“ += np.log(Î»Ë£[d_i])

        Î»Ë£ += Î±[d_i,:]
        t_prev = t_i

    â„“ -= np.sum(mutual_exp_hawkes_compensator(T, times, ids, ğ›‰))

    return â„“


def mutual_exp_simulate_by_thinning(ğ›‰, T):

    """
    The Î» is an m-vector which shows the starting intensity for
    each process.

    Each Î±[i] is an m-vector which shows the jump in intensity
    for each of the processes when an arrival comes to process i.

    The Î² is an m-vector which shows the intensity decay rates for
    each processes intensity.
    """
    Î», Î±, Î² = ğ›‰
    m = len(Î»)

    Î»Ë£ = Î»
    times = []

    t = 0

    while True:
        M = np.sum(Î»Ë£)
        Î”t = rnd.exponential() / M
        t += Î”t
        if t > T:
            break

        Î»Ë£ = Î» + (Î»Ë£ - Î») * np.exp(-Î² * Î”t)

        u = M * rnd.rand()
        if u > np.sum(Î»Ë£):
            continue # No arrivals (they are 'thinned' out)

        cumulativeÎ»Ë£ = 0

        for i in range(m):
            cumulativeÎ»Ë£ += Î»Ë£[i]
            if u < cumulativeÎ»Ë£:
                times.append((t, i))
                Î»Ë£ += Î±[i]
                break

    return times


def flatten_theta(ğ›‰):
    return np.hstack([ğ›‰[0], np.hstack(ğ›‰[1]), ğ›‰[2]])


def unflatten_theta(ğ›‰_flat, m):
    Î» = ğ›‰_flat[:m]
    Î± = ğ›‰_flat[m:(m + m**2)].reshape((m,m))
    Î² = ğ›‰_flat[(m + m**2):]

    return (Î», Î±, Î²)


def mutual_exp_mle(ğ­, ids, T, ğ›‰_start):

    m = len(ğ›‰_start[0])
    ğ›‰_start_flat = flatten_theta(ğ›‰_start)

    def loss(ğ›‰_flat):
        return -mutual_exp_log_likelihood(ğ­, ids, T, unflatten_theta(ğ›‰_flat, m))

    def print_progress(ğ›‰_i, itCount = []):
        itCount.append(None)
        i = len(itCount)

        if i % 100 == 0:
            ll = -loss(ğ›‰_i)
            print(f"Iteration {i} loglikelihood {ll:.2f}")

    res = minimize(loss, ğ›‰_start_flat, options={"disp": True, "maxiter": 100_000},
        callback = print_progress, method = 'Nelder-Mead')

    ğ›‰_mle = unflatten_theta(res.x, m)
    logLike = -res.fun

    return ğ›‰_mle, logLike


# More advanced MLE methods for the exponential case


@njit()
def ozaki_recursion(ğ­, ğ›‰, n):
    """
    Calculate sum_{j=1}^{i-1} t_j^n * exp(-Î² * (t_i - t_j)) recursively
    """
    Î», Î±, Î² = ğ›‰
    N_T = len(ğ­)

    A_n = np.empty(N_T, dtype=np.float64)
    A_n[0] = 0
    for i in range(1, N_T):
        A_n[i] = np.exp(-Î²*(ğ­[i] - ğ­[i-1])) * (ğ­[i-1]**n + A_n[i-1])

    return A_n


@njit()
def deriv_exp_log_likelihood(â„‹_T, T, ğ›‰):
    Î», Î±, Î² = ğ›‰

    ğ­ = â„‹_T
    N_T = len(ğ­)

    A = ozaki_recursion(ğ­, ğ›‰, 0)
    A_1 = ozaki_recursion(ğ­, ğ›‰, 1)

    B = np.empty(N_T, dtype=np.float64)
    B[0] = 0

    for i in range(1, N_T):
        B[i] = ğ­[i] * A[i] - A_1[i]

    dâ„“dÎ» = -T
    dâ„“dÎ± = 0
    dâ„“dÎ² = 0

    for i, t_i in enumerate(â„‹_T):
        dâ„“dÎ± += (1/Î²) * (np.exp(-Î²*(T-t_i)) - 1) + A[i] / (Î» + Î± * A[i])
        dâ„“dÎ² += -Î± * ( (1/Î²) * (T - t_i) * np.exp(-Î²*(T-t_i)) \
                     + (1/Î²**2) * (np.exp(-Î²*(T-t_i))-1) ) \
                - (Î± * B[i] / (Î» + Î± * A[i]))
        dâ„“dÎ» += 1 / (Î» + Î± * A[i])

    d = np.empty(3, dtype=np.float64)
    d[0] = dâ„“dÎ»
    d[1] = dâ„“dÎ±
    d[2] = dâ„“dÎ²
    return d


@njit()
def hess_exp_log_likelihood(â„‹_T, T, ğ›‰):
    Î», Î±, Î² = ğ›‰

    ğ­ = â„‹_T
    N_T = len(ğ­)

    A = ozaki_recursion(ğ­, ğ›‰, 0)
    A_1 = ozaki_recursion(ğ­, ğ›‰, 1)
    A_2 = ozaki_recursion(ğ­, ğ›‰, 2)

    # B is sum (t_i - t_j) * exp(- ...)
    # C is sum (t_i - t_j)**2 * exp(- ...)
    B = np.empty(N_T, dtype=np.float64)
    C = np.empty(N_T, dtype=np.float64)
    B[0] = 0
    C[0] = 0

    for i in range(1, N_T):
        B[i] = ğ­[i] * A[i] - A_1[i]
        C[i] = ğ­[i]**2 * A[i] - 2*ğ­[i]*A_1[i] + A_2[i]

    d2â„“dÎ±2 = 0
    d2â„“dÎ±dÎ² = 0
    d2â„“dÎ²2 = 0

    d2â„“dÎ»2 = 0
    d2â„“dÎ±dÎ» = 0
    d2â„“dÎ²dÎ» = 0

    for i, t_i in enumerate(â„‹_T):
        d2â„“dÎ±2 += - ( A[i] / (Î» + Î± * A[i]) )**2
        d2â„“dÎ±dÎ² += - ( (1/Î²) * (T - t_i) * np.exp(-Î²*(T-t_i)) \
                     + (1/Î²**2) * (np.exp(-Î²*(T-t_i))-1) ) \
                   + ( -B[i]/(Î» + Î± * A[i]) + (Î± * A[i] * B[i]) / (Î» + Î± * A[i])**2 )

        d2â„“dÎ²2 += Î± * ( (1/Î²) * (T - t_i)**2 * np.exp(-Î²*(T-t_i)) + \
                        (2/Î²**2) * (T - t_i) * np.exp(-Î²*(T-t_i)) + \
                        (2/Î²**3) * (np.exp(-Î²*(T-t_i)) - 1) ) + \
                  ( Î±*C[i] / (Î» + Î± * A[i]) - (Î±*B[i] / (Î» + Î± * A[i]))**2 )


        d2â„“dÎ»2 += -1 / (Î» + Î± * A[i])**2
        d2â„“dÎ±dÎ» += -A[i] / (Î» + Î± * A[i])**2
        d2â„“dÎ²dÎ» += Î± * B[i] / (Î» + Î± * A[i])**2

    H = np.empty((3,3), dtype=np.float64)
    H[0,0] = d2â„“dÎ»2
    H[1,1] = d2â„“dÎ±2
    H[2,2] = d2â„“dÎ²2
    H[0,1] = H[1,0] = d2â„“dÎ±dÎ»
    H[0,2] = H[2,0] = d2â„“dÎ²dÎ»
    H[1,2] = H[2,1] = d2â„“dÎ±dÎ²
    return H


def exp_mle_with_grad(ğ­, T, ğ›‰_start=np.array([1.0, 2.0, 3.0])):
    eps = 1e-5
    ğ›‰_bounds = ((eps, None), (eps, None), (eps, None))
    loss = lambda ğ›‰: -exp_log_likelihood(ğ­, T, ğ›‰)
    grad = lambda ğ›‰: -deriv_exp_log_likelihood(ğ­, T, ğ›‰)
    ğ›‰_mle = minimize(loss, ğ›‰_start, bounds=ğ›‰_bounds, jac=grad).x

    return ğ›‰_mle


def exp_mle_with_hess(ğ­, T, ğ›‰_start=np.array([1.0, 2.0, 3.0])):
    eps = 1e-5
    ğ›‰_bounds = ((eps, None), (eps, None), (eps, None))
    loss = lambda ğ›‰: -exp_log_likelihood(ğ­, T, ğ›‰)
    grad = lambda ğ›‰: -deriv_exp_log_likelihood(ğ­, T, ğ›‰)
    hess = lambda ğ›‰: -hess_exp_log_likelihood(ğ­, T, ğ›‰)
    ğ›‰_mle = minimize(loss, ğ›‰_start, bounds=ğ›‰_bounds, jac=grad, hess=hess,
        method="trust-constr").x

    return ğ›‰_mle


# Alternative simulation method


@njit(nogil=True)
def exp_simulate_by_composition_alt(ğ›‰, T):
    Î», Î±, Î² = ğ›‰
    Î»Ë£_k = Î»
    t_k = 0

    â„‹ = []
    while t_k < T:
        U_1 = rnd.rand()
        U_2 = rnd.rand()

        T_1 = t_k - np.log(U_1) / Î»
        T_2 = t_k - np.log(1 + Î²/(Î»Ë£_k + Î± - Î»)*np.log(U_2))/Î²

        t_prev = t_k
        t_k = min(T_1, T_2)
        â„‹.append(t_k)

        Î»Ë£_k = Î» + (Î»Ë£_k + Î± - Î») * (
                np.exp(-Î² * (t_k - t_prev)))

    return np.array(â„‹[:-1])
